{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70638c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bcd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"AAPL\"\n",
    "years = [2025, 2024]\n",
    "quarters = [1, 2, 3, 4]\n",
    "fmp_api_key = \"b6adf265209f12e18fd61e2f403585c3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924693b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_16920\\2618679024.py:22: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  oaiembeds = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from helper import (\n",
    "    processing_html2txt,\n",
    "    combine_sentences,\n",
    "    calculate_cosine_distances,\n",
    "    find_appropriate_threshold,\n",
    ")\n",
    "from pdf_to_gcp import HtmlToPdfGcpUploader\n",
    "from sec_downloader import Downloader\n",
    "\n",
    "# MongoDB setup\n",
    "mongo_client = MongoClient(os.getenv(\"MONGO_URI\"))\n",
    "db = mongo_client[\"qualitative\"]\n",
    "collection = db[\"earnings\"]\n",
    "\n",
    "# Embedding model (OpenAI)\n",
    "oaiembeds = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "dl = Downloader(\"Traderware\", \"x.tan@traderverse.io\")\n",
    "\n",
    "def process_and_insert_to_mongodb(file):\n",
    "    sentence_texts = re.split(r\"(?<=[.#:])\\s+\", file[0].get(\"content\", \"\"))\n",
    "    sentences = [{\"sentence\": s, \"index\": i} for i, s in enumerate(sentence_texts)]\n",
    "    sentences = combine_sentences(sentences)\n",
    "\n",
    "    # Step 2: Embed each sentence (OpenAI)\n",
    "    embeddings = oaiembeds.embed_documents(\n",
    "        [x[\"combined_sentence\"] for x in sentences]\n",
    "    )\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        sentences[i][\"combined_sentence_embedding\"] = emb\n",
    "\n",
    "    # Step 3: Semantic chunking\n",
    "    distances, sentences = calculate_cosine_distances(sentences)\n",
    "    \n",
    "    threshold, chunks, chunk_sizes = find_appropriate_threshold(sentences, distances, 95, 1000)\n",
    "    breakpoint_distance_threshold = np.percentile(distances, threshold)\n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold]\n",
    "\n",
    "    # Step 4: Group sentences into chunks\n",
    "    chunk_texts = []\n",
    "    start_index = 0\n",
    "    for index in indices_above_thresh:\n",
    "        group = sentences[start_index : index + 1]\n",
    "        chunk_texts.append(\" \".join([d[\"sentence\"] for d in group]))\n",
    "        start_index = index + 1\n",
    "    if start_index < len(sentences):\n",
    "        chunk_texts.append(\" \".join([d[\"sentence\"] for d in sentences[start_index:]]))\n",
    "\n",
    "    # Step 5: Embed chunks with OpenAI\n",
    "    chunk_embeddings = oaiembeds.embed_documents(chunk_texts)\n",
    "\n",
    "    # Step 6: Insert into MongoDB\n",
    "    safe_date = file[0].get(\"date\", \"\")\n",
    "    for chunk, vector in zip(chunk_texts, chunk_embeddings):\n",
    "        doc = {\n",
    "            \"content\": chunk,\n",
    "            \"embedding\": vector,\n",
    "            \"file_name\": \n",
    "                f\"{file[0].get(\"symbol\", \"\")}_{file[0].get(\"period\", \"\")}_{safe_date}\",\n",
    "            \"ticker\": file[0].get(\"symbol\", \"\"),\n",
    "            \"quarter\": file[0].get(\"period\", \"\"),\n",
    "            \"date\": datetime.fromisoformat(safe_date)\n",
    "        }\n",
    "        collection.insert_one(doc)\n",
    "\n",
    "    print(f\"✅ Inserted {len(chunk_texts)} chunks using OpenAI embeddings for: {file[0].get(\"symbol\", \"\")}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110b3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL Q1 2025...\n",
      "✅ Inserted 81 chunks using OpenAI embeddings for: AAPL\n",
      "Fetching AAPL Q2 2025...\n",
      "No data for Q2 2025. Skipping.\n",
      "Fetching AAPL Q3 2025...\n",
      "No data for Q3 2025. Skipping.\n",
      "Fetching AAPL Q4 2025...\n",
      "No data for Q4 2025. Skipping.\n",
      "Fetching AAPL Q1 2024...\n",
      "✅ Inserted 136 chunks using OpenAI embeddings for: AAPL\n",
      "Fetching AAPL Q2 2024...\n",
      "✅ Inserted 114 chunks using OpenAI embeddings for: AAPL\n",
      "Fetching AAPL Q3 2024...\n",
      "✅ Inserted 77 chunks using OpenAI embeddings for: AAPL\n",
      "Fetching AAPL Q4 2024...\n",
      "✅ Inserted 81 chunks using OpenAI embeddings for: AAPL\n"
     ]
    }
   ],
   "source": [
    "# Ingest\n",
    "for year in years:\n",
    "    for quarter in quarters:\n",
    "        print(f\"Fetching {symbol} Q{quarter} {year}...\")\n",
    "        url = f\"https://financialmodelingprep.com/stable/earning-call-transcript?symbol={symbol}&year={year}&quarter={quarter}&apikey={fmp_api_key}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"No data for Q{quarter} {year}. Skipping.\")\n",
    "            continue\n",
    "        transcript = data[0].get(\"content\", \"\")\n",
    "        if not transcript:\n",
    "            print(f\"No transcript content for Q{quarter} {year}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            process_and_insert_to_mongodb(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file[0].get(\"date\", \"\")}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb2011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0808889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance, PayloadSchemaType\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from helper import (\n",
    "    processing_html2txt,\n",
    "    combine_sentences,\n",
    "    calculate_cosine_distances,\n",
    "    find_appropriate_threshold,\n",
    ")\n",
    "from sec_downloader import Downloader\n",
    "\n",
    "# Qdrant setup\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Embedding model\n",
    "oaiembeds = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "dl = Downloader(\"Traderware\", \"x.tan@traderverse.io\")\n",
    "\n",
    "def process_and_insert_earnings_to_qdrant(file):\n",
    "    # 1️⃣ Sentence splitting\n",
    "    sentence_texts = re.split(r\"(?<=[.#:])\\s+\", file[0].get(\"content\", \"\"))\n",
    "    sentences = [{\"sentence\": s, \"index\": i} for i, s in enumerate(sentence_texts)]\n",
    "    sentences = combine_sentences(sentences)\n",
    "\n",
    "    # 2️⃣ Embed sentences for semantic chunking\n",
    "    sent_embeds = oaiembeds.embed_documents([s[\"combined_sentence\"] for s in sentences])\n",
    "    for i, emb in enumerate(sent_embeds):\n",
    "        sentences[i][\"combined_sentence_embedding\"] = emb\n",
    "\n",
    "    # 3️⃣ Semantic chunking\n",
    "    distances, sentences = calculate_cosine_distances(sentences)\n",
    "    threshold, _, _ = find_appropriate_threshold(sentences, distances, 95, 1000)\n",
    "    break_idx = np.percentile(distances, threshold)\n",
    "    boundaries = [i for i, d in enumerate(distances) if d > break_idx]\n",
    "\n",
    "    chunk_texts = []\n",
    "    start = 0\n",
    "    for b in boundaries:\n",
    "        chunk_texts.append(\" \".join(s[\"sentence\"] for s in sentences[start : b + 1]))\n",
    "        start = b + 1\n",
    "    if start < len(sentences):\n",
    "        chunk_texts.append(\" \".join(s[\"sentence\"] for s in sentences[start:]))\n",
    "\n",
    "    # 4️⃣ Embed chunks\n",
    "    chunk_embeddings = oaiembeds.embed_documents(chunk_texts)\n",
    "\n",
    "    # 5️⃣ Ensure collection + payload index exists\n",
    "    collection_name = \"earnings\"\n",
    "    if not qdrant_client.collection_exists(collection_name):\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=len(chunk_embeddings[0]),\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        qdrant_client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=\"date\",\n",
    "            field_schema=PayloadSchemaType.DATETIME,\n",
    "            wait=True\n",
    "        )\n",
    "\n",
    "    # 6️⃣ Parse & normalize date to \"YYYY-MM-DDThh:mm:ssZ\"\n",
    "    raw_date = file[0].get(\"date\", \"\")\n",
    "    safe_date_iso = None\n",
    "    if raw_date:\n",
    "        try:\n",
    "            dt = datetime.strptime(raw_date, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            dt = datetime.fromisoformat(raw_date)\n",
    "        safe_date_iso = dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # 7️⃣ Build and upsert points\n",
    "    base_id = int(datetime.now().timestamp() * 1000)\n",
    "    points = []\n",
    "    for idx, (chunk, vector) in enumerate(zip(chunk_texts, chunk_embeddings)):\n",
    "        points.append(\n",
    "            PointStruct(\n",
    "                id=base_id + idx,\n",
    "                vector=vector,\n",
    "                payload={\n",
    "                    \"content\":   chunk,\n",
    "                    \"file_name\": f\"{file[0].get('symbol','')}_{file[0].get('period','')}_{safe_date_iso or ''}\",\n",
    "                    \"ticker\":    file[0].get(\"symbol\", \"\"),\n",
    "                    \"quarter\":   file[0].get(\"period\", \"\"),\n",
    "                    \"date\":      safe_date_iso,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "    print(f\"✅ Inserted {len(points)} earnings chunks for: {file[0].get('symbol','')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c7f04f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL Q1 2025...\n",
      "✅ Inserted 81 earnings chunks for: AAPL\n",
      "Fetching AAPL Q2 2025...\n",
      "No data for Q2 2025. Skipping.\n",
      "Fetching AAPL Q3 2025...\n",
      "No data for Q3 2025. Skipping.\n",
      "Fetching AAPL Q4 2025...\n",
      "No data for Q4 2025. Skipping.\n",
      "Fetching AAPL Q1 2024...\n",
      "✅ Inserted 136 earnings chunks for: AAPL\n",
      "Fetching AAPL Q2 2024...\n",
      "✅ Inserted 114 earnings chunks for: AAPL\n",
      "Fetching AAPL Q3 2024...\n",
      "✅ Inserted 77 earnings chunks for: AAPL\n",
      "Fetching AAPL Q4 2024...\n",
      "✅ Inserted 81 earnings chunks for: AAPL\n"
     ]
    }
   ],
   "source": [
    "# Ingest\n",
    "for year in years:\n",
    "    for quarter in quarters:\n",
    "        print(f\"Fetching {symbol} Q{quarter} {year}...\")\n",
    "        url = f\"https://financialmodelingprep.com/stable/earning-call-transcript?symbol={symbol}&year={year}&quarter={quarter}&apikey={fmp_api_key}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"No data for Q{quarter} {year}. Skipping.\")\n",
    "            continue\n",
    "        transcript = data[0].get(\"content\", \"\")\n",
    "        if not transcript:\n",
    "            print(f\"No transcript content for Q{quarter} {year}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            process_and_insert_earnings_to_qdrant(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {data[0].get(\"date\", \"\")}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1992f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
